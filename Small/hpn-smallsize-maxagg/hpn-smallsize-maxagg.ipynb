{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport time\nimport argparse\n\nimport os\nimport datetime\n\nfrom torch.distributions.categorical import Categorical\n\n# visualization \n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndevice = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n\ngpu_id = '0' # select a single GPU  \n#gpu_id = '2,3' # select multiple GPUs  \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n    \nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T03:01:51.120921Z","iopub.execute_input":"2021-09-18T03:01:51.121177Z","iopub.status.idle":"2021-09-18T03:01:51.131796Z","shell.execute_reply.started":"2021-09-18T03:01:51.121151Z","shell.execute_reply":"2021-09-18T03:01:51.131115Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"GPU name: Tesla P100-PCIE-16GB, gpu_id: 0\ncuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model's Components","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport torch.nn.functional as F\nimport random\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nclass TransEncoderNet(nn.Module):\n    \"\"\"\n    Encoder network based on self-attention transformer\n    Inputs :  \n      h of size      (bsz, nb_nodes, dim_emb)    batch of input cities\n    Outputs :  \n      h of size      (bsz, nb_nodes, dim_emb)    batch of encoded cities\n      score of size  (bsz, nb_nodes, nb_nodes+1) batch of attention scores\n    \"\"\"\n    \n    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n        super(TransEncoderNet, self).__init__()\n        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n        if batchnorm:\n            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n        else:\n            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n        self.nb_layers = nb_layers\n        self.nb_heads = nb_heads\n        self.batchnorm = batchnorm\n        \n    def forward(self, h):      \n        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n        # L layers\n        for i in range(self.nb_layers):\n            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n            # add residual connection\n            \n            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n            if self.batchnorm:\n                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n            else:\n                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n            # feedforward\n            h_rc = h # residual connection\n            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n            if self.batchnorm:\n                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n            else:\n                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n        # Transpose h\n        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n        return h, score\n    \n\nclass Attention(nn.Module):\n    def __init__(self, n_hidden):\n        super(Attention, self).__init__()\n        self.size = 0\n        self.batch_size = 0\n        self.dim = n_hidden\n        \n        v  = torch.FloatTensor(n_hidden)\n        self.v  = nn.Parameter(v)\n        self.v.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n        \n        # parameters for pointer attention\n        self.Wref = nn.Linear(n_hidden, n_hidden)\n        self.Wq = nn.Linear(n_hidden, n_hidden)\n    \n    \n    def forward(self, q, ref):       # query and reference\n        self.batch_size = q.size(0)\n        self.size = int(ref.size(0) / self.batch_size)\n        q = self.Wq(q)     # (B, dim)\n        ref = self.Wref(ref)\n        ref = ref.view(self.batch_size, self.size, self.dim)  # (B, size, dim)\n        \n        q_ex = q.unsqueeze(1).repeat(1, self.size, 1) # (B, size, dim)\n        # v_view: (B, dim, 1)\n        v_view = self.v.unsqueeze(0).expand(self.batch_size, self.dim).unsqueeze(2)\n        \n        # (B, size, dim) * (B, dim, 1)\n        u = torch.bmm(torch.tanh(q_ex + ref), v_view).squeeze(2)\n        \n        return u, ref\n    \nclass LSTM(nn.Module):\n    def __init__(self, n_hidden):\n        super(LSTM, self).__init__()\n        \n        # parameters for input gate\n        self.Wxi = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Whi = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        self.wci = nn.Linear(n_hidden, n_hidden)    # w(ct)\n        \n        # parameters for forget gate\n        self.Wxf = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Whf = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        self.wcf = nn.Linear(n_hidden, n_hidden)    # w(ct)\n        \n        # parameters for cell gate\n        self.Wxc = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Whc = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        \n        # parameters for forget gate\n        self.Wxo = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Who = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        self.wco = nn.Linear(n_hidden, n_hidden)    # w(ct)\n    \n    \n    def forward(self, x, h, c):       # query and reference\n        \n        # input gate\n        i = torch.sigmoid(self.Wxi(x) + self.Whi(h) + self.wci(c))\n        # forget gate\n        f = torch.sigmoid(self.Wxf(x) + self.Whf(h) + self.wcf(c))\n        # cell gate\n        c = f * c + i * torch.tanh(self.Wxc(x) + self.Whc(h))\n        # output gate\n        o = torch.sigmoid(self.Wxo(x) + self.Who(h) + self.wco(c))\n        \n        h = o * torch.tanh(c)\n        \n        return h, c\n\nclass HPN(nn.Module):\n    def __init__(self, n_feature, n_hidden):\n\n        super(HPN, self).__init__()\n        self.city_size = 0\n        self.batch_size = 0\n        self.dim = n_hidden\n        \n        # lstm for first turn\n        #self.lstm0 = nn.LSTM(n_hidden, n_hidden)\n        \n        # pointer layer\n        self.pointer = Attention(n_hidden)\n        self.TransPointer = Attention(n_hidden)\n        \n        # lstm encoder\n        self.encoder = LSTM(n_hidden)\n        \n        # trainable first hidden input\n        h0 = torch.FloatTensor(n_hidden)\n        c0 = torch.FloatTensor(n_hidden)\n        \n        # trainable latent variable coefficient\n        alpha = torch.ones(1).cuda()\n        \n        self.h0 = nn.Parameter(h0)\n        self.c0 = nn.Parameter(c0)\n        \n        self.alpha = nn.Parameter(alpha)\n        self.h0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n        self.c0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n        \n        r1 = torch.ones(1)\n        r2 = torch.ones(1)\n        r3 = torch.ones(1)\n        self.r1 = nn.Parameter(r1)\n        self.r2 = nn.Parameter(r2)\n        self.r3 = nn.Parameter(r3)\n        \n        # embedding\n        self.embedding_x = nn.Linear(n_feature, n_hidden)\n        self.embedding_all = nn.Linear(n_feature, n_hidden)\n        self.Transembedding_all = TransEncoderNet(6, 128, 8, 512, batchnorm=True)\n        \n        # vector to start decoding \n        self.start_placeholder = nn.Parameter(torch.randn(n_hidden))\n        \n        # weights for GNN\n        self.W1 = nn.Linear(n_hidden, n_hidden)\n        self.W2 = nn.Linear(n_hidden, n_hidden)\n        self.W3 = nn.Linear(n_hidden, n_hidden)\n        \n        # aggregation function for GNN\n        self.agg_1 = nn.Linear(n_hidden, n_hidden)\n        self.agg_2 = nn.Linear(n_hidden, n_hidden)\n        self.agg_3 = nn.Linear(n_hidden, n_hidden)\n    \n    \n    def forward(self,context,Transcontext, x, X_all, mask, h=None, c=None, latent=None):\n        '''\n        Inputs (B: batch size, size: city size, dim: hidden dimension)\n        \n        x: current city coordinate (B, 2)\n        X_all: all cities' cooridnates (B, size, 2)\n        mask: mask visited cities\n        h: hidden variable (B, dim)\n        c: cell gate (B, dim)\n        latent: latent pointer vector from previous layer (B, size, dim)\n        \n        Outputs\n        \n        softmax: probability distribution of next city (B, size)\n        h: hidden variable (B, dim)\n        c: cell gate (B, dim)\n        latent_u: latent pointer vector for next layer\n        '''\n        \n        self.batch_size = X_all.size(0)\n        self.city_size = X_all.size(1)\n        \n        # Check if this the first iteration loop\n        if h is None or c is None:\n            x          = self.start_placeholder    \n            context = self.embedding_all(X_all)\n            Transcontext,_ = self.Transembedding_all(context)\n            \n            # =============================\n            # graph neural network encoder\n            # =============================\n\n            # (B, size, dim)\n            context = context.reshape(-1, self.dim)\n            Transcontext = Transcontext.reshape(-1, self.dim)\n\n            context = self.r1 * self.W1(context)\\\n                + (1-self.r1) * F.relu(self.agg_1(context/(self.city_size-1)))\n\n            context = self.r2 * self.W2(context)\\\n                + (1-self.r2) * F.relu(self.agg_2(context/(self.city_size-1)))\n\n            context = self.r3 * self.W3(context)\\\n                + (1-self.r3) * F.relu(self.agg_3(context/(self.city_size-1)))\n            h0 = self.h0.unsqueeze(0).expand(self.batch_size, self.dim)\n            c0 = self.c0.unsqueeze(0).expand(self.batch_size, self.dim)\n\n            h0 = h0.unsqueeze(0).contiguous()\n            c0 = c0.unsqueeze(0).contiguous()\n            \n            # let h0, c0 be the hidden variable of first turn\n            h = h0.squeeze(0)\n            c = c0.squeeze(0)\n        else:\n            x          = self.embedding_x(x)\n        # LSTM encoder\n        h, c = self.encoder(x, h, c)\n        # query vector\n        q = h\n        # pointer\n        u1, _ = self.pointer(q, context)\n        u2 ,_ = self.TransPointer(q,Transcontext)\n        \n        # Max betweenthe two attention vectors\n        u = torch.maximum(u1,u2)\n        latent_u = u.clone()\n        u = 10 * torch.tanh(u) + mask\n        return context,Transcontext,F.softmax(u, dim=1), h, c, latent_u","metadata":{"execution":{"iopub.status.busy":"2021-09-18T03:01:51.258540Z","iopub.execute_input":"2021-09-18T03:01:51.258987Z","iopub.status.idle":"2021-09-18T03:01:51.305269Z","shell.execute_reply.started":"2021-09-18T03:01:51.258959Z","shell.execute_reply":"2021-09-18T03:01:51.304602Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Main Training Cell","metadata":{}},{"cell_type":"code","source":"size = 50\nTOL  =  1e-3\nTINY =  1e-15\nlearn_rate = 1e-4    # learning rate\nB = 512              # batch_size\nB_val = 1000         # validation Batchsize\nB_valLoop = 20       \nsize_val = 50       \nsteps = 2500        # training steps\nn_epoch = 100       # epochs\n\nprint('=========================')\nprint('prepare to train')\nprint('=========================')\nprint('Hyperparameters:')\nprint('size', size)\nprint('size_val', size_val)\nprint('learning rate', learn_rate)\nprint('batch size', B)\nprint('validation size', B_val)\nprint('steps', steps)\nprint('epoch', n_epoch)\nprint('=========================')\n\n###################\n# Instantiate a training network and a baseline network\n###################\n\nX_val = torch.rand(B_val,size_val,2).to(device)\n\ntry: \n    del Actor # remove existing model\n    del Critic # remove existing model\nexcept:\n    pass\n\nActor  = HPN(n_feature=2, n_hidden=128)\nCritic = HPN(n_feature=2, n_hidden=128)\noptimizer = optim.Adam(Actor.parameters(), lr=learn_rate)\n\n# Putting Critic model on the eval mode\nActor = Actor.to(device)\nCritic = Critic.to(device)\nCritic.eval()\n\n########################\n# Remember to first initialize the model and optimizer, then load the dictionary locally.\n#######################\nepoch_ckpt = 0\ntot_time_ckpt = 0\n\nval_mean = []\nval_std  = []\n\nplot_performance_train = []\nplot_performance_baseline = []\n#********************************************# Uncomment these lines to re-start training with saved checkpoint #********************************************#\n\"\"\"\ncheckpoint_file = \"./checkpoint/checkpoint_21-08-02--03-01-00-n50-gpu0.pkl\"\ncheckpoint = torch.load(checkpoint_file, map_location=device)\nepoch_ckpt = checkpoint['epoch'] + 1\ntot_time_ckpt = checkpoint['tot_time']\nplot_performance_train = checkpoint['plot_performance_train']\nplot_performance_baseline = checkpoint['plot_performance_baseline']\nCritic.load_state_dict(checkpoint['model_baseline'])\nActor.load_state_dict(checkpoint['model_train'])\noptimizer.load_state_dict(checkpoint['optimizer'])\n\nprint('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\ndel checkpoint\n\"\"\"\n#*********************************************# Uncomment these lines to re-start training with saved checkpoint #********************************************#\n\n\n###################\n#  Main training loop \n#  The core training  concept mainly upon Sampling from the actor & taking the greedy action from the critic\n###################\n\nstart_training_time = time.time()\ntime_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n\nC = 0     # baseline\nR = 0     # reward\n\nzero_to_bsz = torch.arange(B, device=device) # [0,1,...,bsz-1]\n\nfor epoch in range(0,n_epoch):\n    # re-start training with saved checkpoint\n    epoch += epoch_ckpt\n\n    ###################\n    # Train model for one epoch\n    ###################\n    \n    start = time.time()\n    Actor.train()\n    \n    for i in range(1,steps+1):\n        \n        X = torch.rand(B, size, 2).cuda()                \n        mask = torch.zeros(B,size).cuda()\n        R = 0\n        logprobs = 0\n        reward = 0\n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        h = None\n        c = None\n        context = None\n        Transcontext = None \n        \n        #Actor Sampling phase\n        for k in range(size):\n            context,Transcontext,output, h, c, _ = Actor(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)            \n            sampler = torch.distributions.Categorical(output)\n            idx = sampler.sample()         \n            Y1 = Y[zero_to_bsz, idx.data].clone()\n            if k == 0:\n                Y_ini = Y1.clone()\n            if k > 0:\n                reward = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n            Y0 = Y1.clone()  # --> insert current node into prev node for the next iteration\n            x = Y[zero_to_bsz, idx.data].clone()\n            R += reward\n            logprobs += torch.log(output[zero_to_bsz, idx.data] + TINY)\n            mask[zero_to_bsz, idx.data] += -np.inf    \n        R += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n       \n       \n        # Critic Baseline phase\n        mask = torch.zeros(B,size).cuda()\n        C = 0\n        baseline = 0\n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        h = None\n        c = None\n        context = None\n        Transcontext = None\n\n        # compute tours for baseline without grad \"Cause we want to fix the weights for the critic\"\n        with torch.no_grad():\n            for k in range(size):\n                context,Transcontext,output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n                idx = torch.argmax(output, dim=1) # ----> greedy baseline critic\n                Y1 = Y[zero_to_bsz, idx.data].clone()\n                if k == 0:\n                    Y_ini = Y1.clone()\n                if k > 0:\n                    baseline  = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n                Y0 = Y1.clone()\n                x = Y[zero_to_bsz, idx.data].clone()\n                C += baseline\n                mask[zero_to_bsz, idx.data] += -np.inf\n        C  += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n       \n        ###################\n        # Loss and backprop handling \n        ###################\n        \n        loss = torch.mean((R - C) * logprobs)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if i % 50 == 0:\n            print(\"epoch:{}, batch:{}/{}, reward:{}\".format(epoch, i, steps, R.mean().item()))\n                \n    time_one_epoch = time.time() - start\n    time_tot = time.time() - start_training_time + tot_time_ckpt\n    \n    ###################\n    # Evaluate train model and baseline \n    # in this phase we just solve random instances with the actor and the critic\n    # compare this soluation if we get any improvment we'll transfer the actor's\n    # weights into the critic\n    ###################\n    \n    # putting the actor in the eval mode\n    Actor.eval()\n    \n    mean_tour_length_actor = 0\n    mean_tour_length_critic = 0\n\n    for step in range(0,B_valLoop):\n        \n        # compute tour for model and baseline\n        X = np.random.rand(B, size, 2)        \n        X = torch.Tensor(X).cuda()\n\n        mask = torch.zeros(B,size).cuda()\n        R = 0\n        reward = 0\n\n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        \n        h = None\n        c = None\n        context = None\n        Transcontext = None\n\n        with torch.no_grad():\n            for k in range(size):\n                \n                context,Transcontext,output, h, c, _ = Actor(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)          \n                idx = torch.argmax(output, dim=1)\n\n                Y1 = Y[zero_to_bsz, idx.data].clone()\n                if k == 0:\n                    Y_ini = Y1.clone()\n                if k > 0:\n                    #reward = torch.linalg.norm(Y1 - Y0, dim=1) # --> Calculation of the distance between two node\n                    reward = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n\n                Y0 = Y1.clone()  # --> insert current node into prev node for the next iteration\n                x = Y[zero_to_bsz, idx.data].clone()\n                R += reward\n                mask[zero_to_bsz, idx.data] += -np.inf\n                \n        #R += torch.linalg.norm(Y1 - Y_ini, dim=1)\n        R += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n\n        \n        # critic baseline\n        mask = torch.zeros(B,size).cuda()\n        C = 0\n        baseline = 0\n        \n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        \n        h = None\n        c = None\n        context = None\n        Transcontext = None\n\n        with torch.no_grad():\n            for k in range(size):\n                context,Transcontext,output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n                idx = torch.argmax(output, dim=1)  \n                Y1 = Y[zero_to_bsz, idx.data].clone()\n                if k == 0:\n                    Y_ini = Y1.clone()\n                if k > 0:\n                    #baseline = torch.linalg.norm(Y1-Y0, dim=1)\n                    baseline  = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n                Y0 = Y1.clone()\n                x = Y[zero_to_bsz, idx.data].clone()\n                C += baseline\n                mask[zero_to_bsz, idx.data] += -np.inf\n                \n        #C += torch.linalg.norm(Y1-Y_ini, dim=1) # ---> Last point to intial point\n        C  += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n        \n        mean_tour_length_actor  += R.mean().item()\n        mean_tour_length_critic += C.mean().item()\n        \n    mean_tour_length_actor  =  mean_tour_length_actor  / B_valLoop\n    mean_tour_length_critic =  mean_tour_length_critic / B_valLoop\n\n    # evaluate train model and baseline and update if train model is better\n    update_baseline = mean_tour_length_actor + TOL < mean_tour_length_critic\n    print('Avg Actor {} --- Avg Critic {}'.format(mean_tour_length_actor,mean_tour_length_critic))\n    if update_baseline:\n        Critic.load_state_dict(Actor.state_dict())\n        print('My actor is going on the right road Hallelujah :) Updated')\n        \n    ###################\n    # Valdiation train model and baseline on 1k random TSP instances\n    ###################\n    \n    with torch.no_grad():\n        # greedy validation\n        tour_len = 0\n        X = X_val\n        mask = torch.zeros(B_val,size_val).cuda()\n        R = 0\n        reward = 0\n\n        Y = X.view(B_val, size_val, 2)    # to the same batch size\n        x = Y[:,0,:]\n        \n        h = None\n        c = None\n        context = None\n        Transcontext = None\n\n        for k in range(size_val):\n\n            context,Transcontext,output, h, c, _ = Critic(context,Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n            idx = torch.argmax(output, dim=1)\n            Y1 = Y[[i for i in range(B_val)], idx.data]\n            if k == 0:\n                Y_ini = Y1.clone()\n            if k > 0:\n                #reward = torch.linalg.norm(Y1-Y0, dim=1)\n                reward  = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n            Y0 = Y1.clone()\n            x = Y[[i for i in range(B_val)], idx.data]\n            R += reward\n            mask[[i for i in range(B_val)], idx.data] += -np.inf\n\n        #R += torch.linalg.norm(Y1-Y_ini, dim=1)\n        R  += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n        tour_len += R.mean().item()\n        print('validation tour length:', tour_len)\n\n    # For checkpoint\n    plot_performance_train.append([(epoch+1), mean_tour_length_actor])\n    plot_performance_baseline.append([(epoch+1), mean_tour_length_critic])\n    \n    # Compute optimality gap\n    if size==50: gap_train = mean_tour_length_actor/5.692- 1.0\n    elif size==100: gap_train = mean_tour_length_actor/7.765- 1.0\n    else: gap_train = -1.0\n        \n    # Print and save in txt file\n    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_actor: {:.3f}, L_critic: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_actor, mean_tour_length_critic, 100 * gap_train, update_baseline)\n    \n    print(mystring_min)\n    print('Save Checkpoints')\n    \n    # Saving checkpoint\n    checkpoint_dir = os.path.join(\"checkpoint\")\n    \n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n        \n    torch.save({\n        'epoch': epoch,\n        'time': time_one_epoch,\n        'tot_time': time_tot,\n        'loss': loss.item(),\n        'plot_performance_train': plot_performance_train,\n        'plot_performance_baseline': plot_performance_baseline,\n        'mean_tour_length_val': tour_len,\n        'model_baseline': Critic.state_dict(),\n        'model_train': Actor.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(size) + \"-gpu{}\".format(gpu_id)))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T03:01:51.329956Z","iopub.execute_input":"2021-09-18T03:01:51.330138Z","iopub.status.idle":"2021-09-18T03:02:11.053237Z","shell.execute_reply.started":"2021-09-18T03:01:51.330118Z","shell.execute_reply":"2021-09-18T03:02:11.052018Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"=========================\nprepare to train\n=========================\nHyperparameters:\nsize 50\nsize_val 50\nlearning rate 0.0001\nbatch size 512\nvalidation size 1000\nsteps 1\nepoch 100\n=========================\nAvg Actor 17.28380241394043 --- Avg Critic 16.713729190826417\nvalidation tour length: 16.806711196899414\nEpoch: 0, epoch time: 0.010min, tot time: 0.000day, L_actor: 17.284, L_critic: 16.714, gap_train(%): 203.651, update: False\nSave Checkpoints\nAvg Actor 17.28616018295288 --- Avg Critic 16.681924724578856\nvalidation tour length: 16.806711196899414\nEpoch: 1, epoch time: 0.007min, tot time: 0.000day, L_actor: 17.286, L_critic: 16.682, gap_train(%): 203.692, update: False\nSave Checkpoints\nAvg Actor 17.294433116912842 --- Avg Critic 16.694226837158205\nvalidation tour length: 16.806711196899414\nEpoch: 2, epoch time: 0.007min, tot time: 0.000day, L_actor: 17.294, L_critic: 16.694, gap_train(%): 203.838, update: False\nSave Checkpoints\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_40/1657245704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTranscontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTranscontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mY1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mzero_to_bsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_40/3180639316.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, Transcontext, x, X_all, mask, h, c, latent)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mx\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mTranscontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransembedding_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# =============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_40/3180639316.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mh_rc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;31m# residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMHA_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;31m# add residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4144\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4145\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4146\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}