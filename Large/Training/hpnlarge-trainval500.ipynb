{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport time\nimport argparse\n\nimport os\nimport datetime\nimport torch.nn.functional as F\nimport random\n\nfrom torch.distributions.categorical import Categorical\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm import tqdm_notebook\nfrom tqdm import tqdm_notebook\nimport math\nimport numpy as np\nimport torch\nimport tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance\n# visualization \n%matplotlib inline\nfrom IPython.display import set_matplotlib_formats, clear_output\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('png2x','pdf')\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndevice = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n\ngpu_id = '0' # select a single GPU  \n#gpu_id = '2,3' # select multiple GPUs  \nos.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n    \nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HPN Large","metadata":{}},{"cell_type":"code","source":"def compute_tour_length(x, tour): \n    \"\"\"\n    Compute the length of a batch of tours\n    Inputs : x of size (bsz, nb_nodes, 2) batch of tsp tour instances\n             tour of size (bsz, nb_nodes) batch of sequences (node indices) of tsp tours\n    Output : L of size (bsz,)             batch of lengths of each tsp tour\n    \"\"\"\n    bsz = x.shape[0]\n    nb_nodes = x.shape[1]\n    arange_vec = torch.arange(bsz, device=x.device)\n    first_cities = x[arange_vec, tour[:,0], :] # size(first_cities)=(bsz,2)\n    previous_cities = first_cities\n    L = torch.zeros(bsz, device=x.device)\n    with torch.no_grad():\n        for i in range(1,nb_nodes):\n            current_cities = x[arange_vec, tour[:,i], :] \n            L += torch.sum( (current_cities - previous_cities)**2 , dim=1 )**0.5 # dist(current, previous node) \n            previous_cities = current_cities\n        L += torch.sum((current_cities - first_cities)**2 , dim=1)**0.5 # dist(last, first node)  \n    return L\n\nclass TransEncoderNet(nn.Module):\n    \"\"\"\n    Encoder network based on self-attention transformer\n    Inputs :  \n      h of size      (bsz, nb_nodes+1, dim_emb)    batch of input cities\n    Outputs :  \n      h of size      (bsz, nb_nodes+1, dim_emb)    batch of encoded cities\n      score of size  (bsz, nb_nodes+1, nb_nodes+1) batch of attention scores\n    \"\"\"\n    \n    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n        super(TransEncoderNet, self).__init__()\n        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n        if batchnorm:\n            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n        else:\n            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n        self.nb_layers = nb_layers\n        self.nb_heads = nb_heads\n        self.batchnorm = batchnorm\n        \n    def forward(self, h):      \n        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n        # L layers\n        for i in range(self.nb_layers):\n            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n            # add residual connection\n            \n            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n            if self.batchnorm:\n                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n            else:\n                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n            # feedforward\n            h_rc = h # residual connection\n            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n            if self.batchnorm:\n                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n            else:\n                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n        # Transpose h\n        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n        return h, score\n    \nclass Attention(nn.Module):\n    def __init__(self, n_hidden):\n        super(Attention, self).__init__()\n        self.size = 0\n        self.batch_size = 0\n        self.dim = n_hidden\n        \n        v  = torch.FloatTensor(n_hidden).cuda()\n        self.v  = nn.Parameter(v)\n        self.v.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n        \n        # parameters for pointer attention\n        self.Wref = nn.Linear(n_hidden, n_hidden)\n        self.Wq = nn.Linear(n_hidden, n_hidden)\n    \n    \n    def forward(self, q, ref):       # query and reference\n        self.batch_size = q.size(0)\n        self.size = int(ref.size(0) / self.batch_size)\n        q = self.Wq(q)     # (B, dim)\n        ref = self.Wref(ref)\n        ref = ref.view(self.batch_size, self.size, self.dim)  # (B, size, dim)\n        \n        q_ex = q.unsqueeze(1).repeat(1, self.size, 1) # (B, size, dim)\n        # v_view: (B, dim, 1)\n        v_view = self.v.unsqueeze(0).expand(self.batch_size, self.dim).unsqueeze(2)\n        \n        # (B, size, dim) * (B, dim, 1)\n        u = torch.bmm(torch.tanh(q_ex + ref), v_view).squeeze(2)\n        \n        return u, ref\n    \nclass LSTM(nn.Module):\n    def __init__(self, n_hidden):\n        super(LSTM, self).__init__()\n        \n        # parameters for input gate\n        self.Wxi = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Whi = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        self.wci = nn.Linear(n_hidden, n_hidden)    # w(ct)\n        \n        # parameters for forget gate\n        self.Wxf = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Whf = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        self.wcf = nn.Linear(n_hidden, n_hidden)    # w(ct)\n        \n        # parameters for cell gate\n        self.Wxc = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Whc = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        \n        # parameters for forget gate\n        self.Wxo = nn.Linear(n_hidden, n_hidden)    # W(xt)\n        self.Who = nn.Linear(n_hidden, n_hidden)    # W(ht)\n        self.wco = nn.Linear(n_hidden, n_hidden)    # w(ct)\n    \n    \n    def forward(self, x, h, c):       # query and reference\n        \n        # input gate\n        i = torch.sigmoid(self.Wxi(x) + self.Whi(h) + self.wci(c))\n        # forget gate\n        f = torch.sigmoid(self.Wxf(x) + self.Whf(h) + self.wcf(c))\n        # cell gate\n        c = f * c + i * torch.tanh(self.Wxc(x) + self.Whc(h))\n        # output gate\n        o = torch.sigmoid(self.Wxo(x) + self.Who(h) + self.wco(c))\n        \n        h = o * torch.tanh(c)\n        \n        return h, c\n\nclass HPN(nn.Module):\n    def __init__(self, n_feature, n_hidden):\n\n        super(HPN, self).__init__()\n        self.city_size = 0\n        self.batch_size = 0\n        self.dim = n_hidden\n        \n        # pointer layer\n        self.pointer = Attention(n_hidden)\n        self.TransPointer = Attention(n_hidden)\n        \n        # lstm encoder\n        self.encoder = LSTM(n_hidden)\n        \n        # trainable first hidden input\n        h0 = torch.FloatTensor(n_hidden)\n        c0 = torch.FloatTensor(n_hidden)\n    \n        self.h0 = nn.Parameter(h0)\n        self.c0 = nn.Parameter(c0)\n        \n        self.h0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n        self.c0.data.uniform_(-1/math.sqrt(n_hidden), 1/math.sqrt(n_hidden))\n        \n        r1 = torch.ones(1)\n        r2 = torch.ones(1)\n        r3 = torch.ones(1)\n        \n        self.r1 = nn.Parameter(r1)\n        self.r2 = nn.Parameter(r2)\n        self.r3 = nn.Parameter(r3)\n        \n        # embedding\n        self.embedding_x = nn.Linear(n_feature, n_hidden)\n        self.embedding_all1 = nn.Linear(n_feature, n_hidden)\n        self.embedding_all2 = nn.Linear(n_feature + 1, n_hidden)\n        self.Transembedding_all = TransEncoderNet(6, 128, 8, 512, batchnorm=True)\n        \n        # vector to start decoding \n        self.start_placeholder = nn.Parameter(torch.randn(n_hidden))\n        \n        # weights for GNN\n        self.W1 = nn.Linear(n_hidden, n_hidden)\n        self.W2 = nn.Linear(n_hidden, n_hidden)\n        self.W3 = nn.Linear(n_hidden, n_hidden)\n        \n        # aggregation function for GNN\n        self.agg_1 = nn.Linear(n_hidden, n_hidden)\n        self.agg_2 = nn.Linear(n_hidden, n_hidden)\n        self.agg_3 = nn.Linear(n_hidden, n_hidden)\n    \n    \n    def forward(self, Transcontext, x, X_all, mask, h=None, c=None, latent=None):\n        '''\n        Inputs (B: batch size, size: city size, dim: hidden dimension)\n        \n        x: current city coordinate (B, 2)\n        X_all: all cities' cooridnates (B, size, 2)\n        mask: mask visited cities\n        h: hidden variable (B, dim)\n        c: cell gate (B, dim)\n        latent: latent pointer vector from previous layer (B, size, dim)\n        \n        Outputs\n        \n        softmax: probability distribution of next city (B, size)\n        h: hidden variable (B, dim)\n        c: cell gate (B, dim)\n        latent_u: latent pointer vector for next layer\n        '''\n        \n        self.batch_size = X_all.size(0)\n        self.city_size = X_all.size(1)\n        \n        # Check if this iteration is the first one\n        if h is None or c is None:\n            # Letting the placeholder be the first input\n            x          = self.start_placeholder\n            #  init-embedding for All Cities\n            context = self.embedding_all1(X_all)\n            # Transormer context \n            Transcontext,_ = self.Transembedding_all(context)\n            \n            Transcontext = Transcontext.reshape(-1, self.dim) # (B, size, dim)\n            \n            # =============================\n            # handling the cell and the hidden state for the first iteration \n            # =============================\n            h0 = self.h0.unsqueeze(0).expand(self.batch_size, self.dim)\n            c0 = self.c0.unsqueeze(0).expand(self.batch_size, self.dim)\n            h0 = h0.unsqueeze(0).contiguous()\n            c0 = c0.unsqueeze(0).contiguous()\n            # let h0, c0 be the hidden variable of first turn\n            h = h0.squeeze(0)\n            c = c0.squeeze(0)\n        else:\n            # =============================\n            # Feature context\n            # =============================\n            X_all      = torch.cat((torch.cdist(X_all,x.view(self.batch_size,1,2),p=2), X_all - x.unsqueeze(1).repeat(1, self.city_size, 1)), 2)\n            # sequential input Embedding \n            x          = self.embedding_x(x)\n            #  init-embedding for All Cities\n            context = self.embedding_all2(X_all)\n            \n        # =============================\n        # graph neural network encoder\n        # =============================\n        # Handling contextes's size\n        context = context.reshape(-1, self.dim)           # (B, size, dim)\n        context = self.r1 * self.W1(context) + (1-self.r1) * F.relu(self.agg_1(context/(self.city_size-1)))\n        context = self.r2 * self.W2(context) + (1-self.r2) * F.relu(self.agg_2(context/(self.city_size-1)))\n        context = self.r3 * self.W3(context) + (1-self.r3) * F.relu(self.agg_3(context/(self.city_size-1)))\n        # LSTM encoder\n        h, c = self.encoder(x, h, c)\n        \n        # =============================\n        # Decoding Phase\n        # ============================= \n        \n        u1, _ = self.pointer(h, context)\n        u2 ,_ = self.TransPointer(h,Transcontext)\n        u = u1 + u2\n        latent_u = u.clone()\n        u = 100 * torch.tanh(u) + mask\n        return Transcontext,F.softmax(u, dim=1), h, c, latent_u","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"size = 50\nTOL  =  1e-3\nTINY =  1e-15\nlearn_rate = 1e-3    # learning rate\nB = 128              # batch_size\nB_val = 64           # validation size\nB_valLoop = 20\nsize_val = 500\nsteps = 2500 # training steps\nn_epoch = 100       # epochs\n\nprint('=========================')\nprint('prepare to train')\nprint('=========================')\nprint('Hyperparameters:')\nprint('size', size)\nprint('size_val', size_val)\nprint('learning rate', learn_rate)\nprint('batch size', B)\nprint('validation size', B_val)\nprint('steps', steps)\nprint('epoch', n_epoch)\nprint('=========================')\n\n###################\n# Instantiate a training network and a baseline network\n###################\n\ntry: \n    del Actor # remove existing model\n    del Critic # remove existing model\nexcept:\n    pass\n\nActor  = HPN(n_feature=2, n_hidden=128)\nCritic = HPN(n_feature=2, n_hidden=128)\noptimizer = optim.Adam(Actor.parameters(), lr=learn_rate)\nlr_decay_step = 2500\nlr_decay_rate = 0.96\nopt_scheduler = lr_scheduler.MultiStepLR(optimizer, range(lr_decay_step, lr_decay_step*1000,lr_decay_step), gamma=lr_decay_rate)\n\n# Putting Critic model on the eval mode\nActor = Actor.to(device)\nCritic = Critic.to(device)\nCritic.eval()\n\n########################\n# Remember to first initialize the model and optimizer, then load the dictionary locally.\n#######################\nepoch_ckpt = 0\ntot_time_ckpt = 0\nplot_performance_train = []\nplot_performance_baseline = []\n\n#********************************************# Uncomment these lines to re-start training with saved checkpoint #********************************************#\n\n#checkpoint_file = \"../input/hpnlarge487e/checkpoint_21-08-08--09-15-31-n50-gpu0.pkl\"\n#checkpoint = torch.load(checkpoint_file, map_location=device)\n#epoch_ckpt = checkpoint['epoch'] + 1\n#tot_time_ckpt = checkpoint['tot_time']\n#plot_performance_train = checkpoint['plot_performance_train']\n#plot_performance_baseline = checkpoint['plot_performance_baseline']\n#Critic.load_state_dict(checkpoint['model_baseline'])\n#Actor.load_state_dict(checkpoint['model_train'])\n#optimizer.load_state_dict(checkpoint['optimizer'])\n\n#print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n#del checkpoint\n\n#*********************************************# Uncomment these lines to re-start training with saved checkpoint #********************************************#\n\n\n###################\n#  Main training loop \n###################\nstart_training_time = time.time()\ntime_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\nC = 0     # baseline\nR = 0     # reward\nzero_to_bsz = torch.arange(B, device=device) # [0,1,...,bsz-1]\nfor epoch in range(0,n_epoch):\n    # re-start training with saved checkpoint\n    epoch += epoch_ckpt\n    ###################\n    # Train model for one epoch\n    ###################\n    start = time.time()\n    Actor.train()\n    for i in range(1,steps+1):\n        X = torch.rand(B, size, 2).cuda()                \n        mask = torch.zeros(B,size).cuda()\n        R = 0\n        logprobs = 0\n        reward = 0\n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        h = None\n        c = None\n        Transcontext = None \n        #Actor Sampling phase\n        for k in range(size):\n            Transcontext,output, h, c, _ = Actor(Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)            \n            sampler = torch.distributions.Categorical(output)\n            idx = sampler.sample()         \n            Y1 = Y[zero_to_bsz, idx.data].clone()\n            if k == 0:\n                Y_ini = Y1.clone()\n            if k > 0:\n                reward = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n            Y0 = Y1.clone()  # --> insert current node into prev node for the next iteration\n            x = Y[zero_to_bsz, idx.data].clone()\n            R += reward\n            logprobs += torch.log(output[zero_to_bsz, idx.data] + TINY)\n            mask[zero_to_bsz, idx.data] += -np.inf    \n        R += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n       \n       \n        # Critic Baseline phase\n        mask = torch.zeros(B,size).cuda()\n        C = 0\n        baseline = 0\n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        h = None\n        c = None\n        Transcontext = None\n        # compute tours for baseline without grad\n        with torch.no_grad():\n            for k in range(size):\n                Transcontext,output, h, c, _ = Critic(Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n                idx = torch.argmax(output, dim=1) # ----> greedy baseline critic\n                Y1 = Y[zero_to_bsz, idx.data].clone()\n                if k == 0:\n                    Y_ini = Y1.clone()\n                if k > 0:\n                    baseline  = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n                Y0 = Y1.clone()\n                x = Y[zero_to_bsz, idx.data].clone()\n                C += baseline\n                mask[zero_to_bsz, idx.data] += -np.inf\n        C  += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n       \n        ###################\n        # Loss and backprop handling \n        ###################\n        \n        loss = torch.mean((R - C) * logprobs)\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(Actor.parameters(),1.0, norm_type=2)\n        optimizer.step()\n        opt_scheduler.step()\n        \n        if i % 50 == 0:\n            print(\"epoch:{}, batch:{}/{}, reward:{}\".format(epoch, i, steps, R.mean().item()))\n            # R_mean.append(R.mean().item())\n            # R_std.append(R.std().item())\n            # greedy validation\n            \n            tour_len = 0\n            X_val = np.random.rand(B_val, size_val, 2)\n            X = X_val\n            X = torch.Tensor(X).cuda()\n            mask = torch.zeros(B_val,size_val).cuda()\n            R = 0\n            logprobs = 0\n            Idx = []\n            reward = 0\n            \n            Y = X.view(B_val, size_val, 2)    # to the same batch size\n            x = Y[:,0,:]\n            h = None\n            c = None\n            Transcontext = None\n            for k in range(size_val):\n                Transcontext,output, h, c, _ = Actor(Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)          \n                sampler = torch.distributions.Categorical(output)\n                # idx = sampler.sample()\n                idx = torch.argmax(output, dim=1)\n                Idx.append(idx.data)\n                Y1 = Y[[i for i in range(B_val)], idx.data]\n                if k == 0:\n                    Y_ini = Y1.clone()\n                if k > 0:\n                    reward = torch.norm(Y1-Y0, dim=1)\n                Y0 = Y1.clone()\n                x = Y[[i for i in range(B_val)], idx.data]\n                R += reward\n                mask[[i for i in range(B_val)], idx.data] += -np.inf\n            R += torch.norm(Y1-Y_ini, dim=1)\n            tour_len += R.mean().item()\n            print('validation tour length:', tour_len)\n            #print('validation tour length:', R.std().item())\n\n                \n    time_one_epoch = time.time() - start\n    time_tot = time.time() - start_training_time + tot_time_ckpt\n    \n    ###################\n    # Evaluate train model and baseline on 1k random TSP instances\n    ###################\n    Actor.eval()    \n    mean_tour_length_actor = 0\n    mean_tour_length_critic = 0\n    for step in range(0,B_valLoop):\n        # compute tour for model and baseline\n        X = np.random.rand(B, size, 2)        \n        X = torch.Tensor(X).cuda()\n        mask = torch.zeros(B,size).cuda()\n        R = 0\n        reward = 0\n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        h = None\n        c = None\n        Transcontext = None\n        with torch.no_grad():\n            for k in range(size):\n                Transcontext,output, h, c, _ = Actor(Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)          \n                idx = torch.argmax(output, dim=1)\n                Y1 = Y[zero_to_bsz, idx.data].clone()\n                if k == 0:\n                    Y_ini = Y1.clone()\n                if k > 0:\n                    #reward = torch.linalg.norm(Y1 - Y0, dim=1) # --> Calculation of the distance between two node\n                    reward = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n                Y0 = Y1.clone()  # --> insert current node into prev node for the next iteration\n                x = Y[zero_to_bsz, idx.data].clone()\n                R += reward\n                mask[zero_to_bsz, idx.data] += -np.inf\n        #R += torch.linalg.norm(Y1 - Y_ini, dim=1)\n        R += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n\n        # critic baseline\n        mask = torch.zeros(B,size).cuda()\n        C = 0\n        baseline = 0\n        Y = X.view(B,size,2)\n        x = Y[:,0,:]\n        h = None\n        c = None\n        Transcontext = None\n        with torch.no_grad():\n            for k in range(size):\n                Transcontext,output, h, c, _ = Critic(Transcontext,x=x, X_all=X, h=h, c=c, mask=mask)\n                idx = torch.argmax(output, dim=1)  \n                Y1 = Y[zero_to_bsz, idx.data].clone()\n                if k == 0:\n                    Y_ini = Y1.clone()\n                if k > 0:\n                    #baseline = torch.linalg.norm(Y1-Y0, dim=1)\n                    baseline  = torch.sum((Y1 - Y0)**2 , dim=1 )**0.5\n                Y0 = Y1.clone()\n                x = Y[zero_to_bsz, idx.data].clone()\n                C += baseline\n                mask[zero_to_bsz, idx.data] += -np.inf\n        #C += torch.linalg.norm(Y1-Y_ini, dim=1) # ---> Last point to intial point\n        C  += torch.sum((Y1 - Y_ini)**2 , dim=1 )**0.5\n        mean_tour_length_actor  += R.mean().item()\n        mean_tour_length_critic += C.mean().item()\n    mean_tour_length_actor  =  mean_tour_length_actor  / B_valLoop\n    mean_tour_length_critic =  mean_tour_length_critic / B_valLoop\n    # evaluate train model and baseline and update if train model is better\n    update_baseline = mean_tour_length_actor + TOL < mean_tour_length_critic\n    print('Avg Actor {} --- Avg Critic {}'.format(mean_tour_length_actor,mean_tour_length_critic))\n    if update_baseline:\n        Critic.load_state_dict(Actor.state_dict())\n        print('My actor is going on the right road Hallelujah :) Updated')\n        \n    # For checkpoint\n    plot_performance_train.append([(epoch+1), mean_tour_length_actor])\n    plot_performance_baseline.append([(epoch+1), mean_tour_length_critic])\n    # Compute optimality gap\n    if size==50: gap_train = mean_tour_length_actor/5.692- 1.0\n    elif size==100: gap_train = mean_tour_length_actor/7.765- 1.0\n    else: gap_train = -1.0\n        \n    # Print and save in txt file\n    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_actor: {:.3f}, L_critic: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_actor, mean_tour_length_critic, 100 * gap_train, update_baseline)\n    \n    print(mystring_min)\n    print('Save Checkpoints')\n    \n    # Saving checkpoint\n    checkpoint_dir = os.path.join(\"checkpoint\")\n    \n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n        \n    torch.save({\n        'epoch': epoch,\n        'time': time_one_epoch,\n        'tot_time': time_tot,\n        'loss': loss.item(),\n        'plot_performance_train': plot_performance_train,\n        'plot_performance_baseline': plot_performance_baseline,\n        'mean_tour_length_val': tour_len,\n        'model_baseline': Critic.state_dict(),\n        'model_train': Actor.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(50) + \"-gpu{}\".format(gpu_id)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}